{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REQUIREMENTS**\n",
    "\n",
    "1. In order to run this notebook, you'll need to provide a HuggingFace Access\n",
    "Token as a Colab Secret named `HF_TOKEN`. This token must also have access to\n",
    "Meta's Llama-2 model series.\n",
    "1. You'll need to change the Colab Runtime to `T4 GPU`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG â€” A Basic Implementation\n",
    "\n",
    "In this notebook, we implement a basic rag system with the `fed-rag` framework,\n",
    "which is an open-sourced library that facilitates both centralized as well as \n",
    "federated fine-tuning of RAG systems.\n",
    "\n",
    "While we won't fine-tune a RAG system in this notebook, we can still make use of\n",
    "the framework to perform inference with RAG systems. Here, we'll use the HuggingFace\n",
    "integration/extra to build a RAG system with a HuggingFace PeftModel as the LLM\n",
    "Generator and a SentenceTransformer as the Retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"fed-rag[huggingface]\" bitsandbytes -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fed_rag.generators.hf_peft_model import HFPeftModelGenerator\n",
    "from transformers.generation.utils import GenerationConfig\n",
    "from transformers.utils.quantization_config import BitsAndBytesConfig\n",
    "\n",
    "PEFT_MODEL_NAME = \"Styxxxx/llama2_7b_lora-quac\"\n",
    "BASE_MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "generation_cfg = GenerationConfig(\n",
    "    do_sample=True,\n",
    "    eos_token_id=[128000, 128009],\n",
    "    bos_token_id=128000,\n",
    "    max_new_tokens=4096,\n",
    "    top_p=0.9,\n",
    "    temperature=0.6,\n",
    "    cache_implementation=\"offloaded\",\n",
    "    stop_strings=\"</response>\",\n",
    ")\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "generator = HFPeftModelGenerator(\n",
    "    model_name=PEFT_MODEL_NAME,\n",
    "    base_model_name=BASE_MODEL_NAME,\n",
    "    generation_config=generation_cfg,\n",
    "    load_model_at_init=False,\n",
    "    load_model_kwargs={\"is_trainable\": True, \"device_map\": \"auto\"},\n",
    "    load_base_model_kwargs={\n",
    "        \"device_map\": \"auto\",\n",
    "        \"quantization_config\": quantization_config,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fed_rag.retrievers.hf_sentence_transformer import (\n",
    "    HFSentenceTransformerRetriever,\n",
    ")\n",
    "\n",
    "QUERY_ENCODER_NAME = \"nthakur/dragon-plus-query-encoder\"\n",
    "CONTEXT_ENCODER_NAME = \"nthakur/dragon-plus-context-encoder\"\n",
    "\n",
    "retriever = HFSentenceTransformerRetriever(\n",
    "    query_model_name=QUERY_ENCODER_NAME,\n",
    "    context_model_name=CONTEXT_ENCODER_NAME,\n",
    "    load_model_at_init=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge Store\n",
    "\n",
    "In this section we build a toy-ish knowledge store from two knowledge facts\n",
    "that are found in the `chunks` list. Using `~fed_rag.knowledge_stores.InMemoryKnowledgeStore`,\n",
    "we load these chunks into a `~fed_rag.KnowledgeStore` type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from fed_rag.knowledge_stores.in_memory import InMemoryKnowledgeStore\n",
    "from fed_rag.types.knowledge_node import KnowledgeNode, NodeType\n",
    "\n",
    "# knowledge chunks\n",
    "chunks_json_strs = [\n",
    "    '{\"id\": \"0\", \"title\": \"Orchid\", \"text\": \"Orchids are easily distinguished from other plants, as they share some very evident derived characteristics or synapomorphies. Among these are: bilateral symmetry of the flower (zygomorphism), many resupinate flowers, a nearly always highly modified petal (labellum), fused stamens and carpels, and extremely small seeds\"}'\n",
    "    '{\"id\": \"1\", \"title\": \"Tulip\", \"text\": \"Tulips are easily distinguished from other plants, as they share some very evident derived characteristics or synapomorphies. Among these are: bilateral symmetry of the flower (zygomorphism), many resupinate flowers, a nearly always highly modified petal (labellum), fused stamens and carpels, and extremely small seeds\"}'\n",
    "]\n",
    "chunks = [json.loads(line) for line in chunks_json_strs]\n",
    "\n",
    "\n",
    "knowledge_store = InMemoryKnowledgeStore()\n",
    "\n",
    "# create knowledge nodes\n",
    "nodes = []\n",
    "for c in chunks:\n",
    "    node = KnowledgeNode(\n",
    "        embedding=retriever.encode_context(c[\"text\"]).tolist(),\n",
    "        node_type=NodeType.TEXT,\n",
    "        text_content=c[\"text\"],\n",
    "        metadata={\"title\": c[\"title\"], \"id\": c[\"id\"]},\n",
    "    )\n",
    "    nodes.append(node)\n",
    "\n",
    "# load into knowledge_store\n",
    "knowledge_store.load_nodes(nodes=nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fed_rag.types.rag_system import RAGConfig, RAGSystem\n",
    "\n",
    "rag_config = RAGConfig(top_k=2)\n",
    "rag_system = RAGSystem(\n",
    "    knowledge_store=knowledge_store,\n",
    "    generator=generator,\n",
    "    retriever=retriever,\n",
    "    rag_config=rag_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query the rag system\n",
    "response = rag_system.query(\"What is a Tulip?\")\n",
    "\n",
    "print(f\"\\n{response}\")\n",
    "\n",
    "# inspect source nodes\n",
    "print(response.source_nodes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pocket-reference-nb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
